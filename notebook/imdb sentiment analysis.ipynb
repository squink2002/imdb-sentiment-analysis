{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "\n",
    "# Sci-Kit Learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# NLTK:\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the IMDb data\n",
    "imdb = pd.read_csv(r\"C:\\Users\\sando\\OneDrive\\Escritorio\\Personal Projects\\IMDB Sentiment Analysis\\dataset\\imdb_reviews.csv\", encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columsn to string\n",
    "imdb['review'] = imdb['review'].astype(str)\n",
    "imdb['sentiment'] = imdb['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if our data is balanced\n",
    "imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review\n",
       "False    50000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NAN values:\n",
    "imdb['review'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the html strips:\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# # Applying our function:\n",
    "# imdb['review'] = imdb['review'].apply(strip_html_tags)\n",
    "# imdb['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words, Punctuation, and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Cleaning up the text:\n",
    "def clean_text(text):\n",
    "    # Tokenizing the text\n",
    "    words = word_tokenize(text)\n",
    "    # Removing stop words, punctuation, and numbers \n",
    "    processed_words = [w for w in words if w.lower() not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    processed_words = ' '.join(processed_words)\n",
    "    return processed_words \n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(clean_text)\n",
    "# print(imdb['review']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Charcaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters:\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(remove_special_characters) \n",
    "# imdb['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Redundant Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the redundant whitespaces:\n",
    "def remove_redundant_whitespaces(text):\n",
    "    text = re.sub(pattern= r'\\s+', repl= ' ', string= text)\n",
    "    return text.strip()\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(remove_redundant_whitespaces)\n",
    "# print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Text Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the text:\n",
    "def simple_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
    "    return stemmed_text\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(simple_stemmer)\n",
    "# print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sando\\AppData\\Local\\Temp\\ipykernel_19980\\1919239933.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one review mention watch oz episod ll hook rig...\n",
      "1        wonder littl product film techniqu unassum old...\n",
      "2        thought wonder way spend time hot summer weeke...\n",
      "3        basic s famili littl boy jake think s zombi cl...\n",
      "4        petter mattei s love time money visual stun fi...\n",
      "                               ...                        \n",
      "49995    thought movi right good job nt creativ origin ...\n",
      "49996    bad plot bad dialogu bad act idiot direct anno...\n",
      "49997    cathol taught parochi elementari school nun ta...\n",
      "49998    m go disagre previou comment side maltin one s...\n",
      "49999    one expect star trek movi high art fan expect ...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    # Removing the HTML tags:\n",
    "    normalized_text = strip_html_tags(text)\n",
    "\n",
    "    # Removing stop words, punctuation, and numbers\n",
    "    normalized_text = clean_text(normalized_text)\n",
    "\n",
    "    # Removing special characters:\n",
    "    normalized_text = remove_special_characters(normalized_text, remove_digits = False)\n",
    "\n",
    "    # Removing the redundant whitespaces:\n",
    "    normalized_text = remove_redundant_whitespaces(normalized_text)\n",
    "\n",
    "    # Stemmatization:\n",
    "    normalized_text = simple_stemmer(normalized_text)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "imdb['review'] = imdb['review'].apply(normalize_text)\n",
    "print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the sentiment data:\n",
    "lb = LabelBinarizer()\n",
    "sentiment_data = lb.fit_transform(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 35000\n",
      "x_test: 15000\n",
      "y_train: 35000\n",
      "y_test: 15000\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset using sklearn\n",
    "X = imdb['review'] # features\n",
    "y = imdb['sentiment'] # target labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.3)\n",
    "\n",
    "print('x_train:', x_train.shape[0])\n",
    "print('x_test:', x_test.shape[0])\n",
    "print('y_train:', y_train.shape[0])\n",
    "print('y_test:', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW_cv_train: (35000, 5958822)\n",
      "BoW_cv_test: (15000, 5958822)\n"
     ]
    }
   ],
   "source": [
    "# Count vectorizer for bag of words:\n",
    "cv = CountVectorizer(min_df= 0.0, # lower threshold\n",
    "                     max_df= 1.0, # upper threshold\n",
    "                     binary= False, \n",
    "                     ngram_range= (1,3)) # ngram_range(1, 3) includes more context\n",
    "# Transforming the training set:\n",
    "cv_train_reviews = cv.fit_transform(x_train)\n",
    "#Tranforming the testing set:\n",
    "cv_test_reviews = cv.transform(x_test)\n",
    "\n",
    "print('BoW_cv_train:', cv_train_reviews.shape)\n",
    "print('BoW_cv_test:', cv_test_reviews.shape)\n",
    "vocab = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Term Frequency-Inverse Document Frequency Model (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (35000, 5958822)\n",
      "Tfidf_test: (15000, 5958822)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(min_df= 0.0, max_df= 1.0, use_idf= True, ngram_range=(1,3))\n",
    "tv_train_reviews = tv.fit_transform(x_train)\n",
    "tv_test_reviews = tv.transform(x_test)\n",
    "\n",
    "print('Tfidf_train:', tv_train_reviews.shape)\n",
    "print('Tfidf_test:', tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow: LogisticRegression(C=1, max_iter=500, random_state=42)\n",
      "lr_tfidf: LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Training the logistic regression model:\n",
    "lr = LogisticRegression(penalty= 'l2', #* L2 Regularization (penalizes the sum of the squared values)\n",
    "                        max_iter= 500, #* maximum number of iterations\n",
    "                        C=1, #* regularization strength\n",
    "                        random_state= 42)\n",
    "\n",
    "# Fitting the model for BoW features:\n",
    "#* variable_name = lr.fit(x_train, y_train)\n",
    "lr_bow = lr.fit(cv_train_reviews, y_train)\n",
    "print('lr_bow:', lr_bow)\n",
    "\n",
    "# Fitting the model for TFIDF feature:\n",
    "#* variable_name = lr.fit(x_train, y_train)\n",
    "lr_tfidf = lr.fit(tv_train_reviews, y_train)\n",
    "print('lr_tfidf:', lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance on the Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
