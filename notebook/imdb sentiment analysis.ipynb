{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "\n",
    "# Sci-Kit Learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# NLTK:\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the IMDb data\n",
    "imdb = pd.read_csv(r\"C:\\Users\\sando\\OneDrive\\Escritorio\\Personal Projects\\IMDB Sentiment Analysis\\dataset\\imdb_reviews.csv\", encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columsn to string\n",
    "imdb['review'] = imdb['review'].astype(str)\n",
    "imdb['sentiment'] = imdb['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if our data is balanced\n",
    "imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the sentiment data:\n",
    "lb = LabelBinarizer()\n",
    "imdb['sentiment'] = lb.fit_transform(imdb['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review\n",
       "False    50000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NAN values:\n",
    "imdb['review'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the html strips:\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# # Applying our function:\n",
    "# imdb['review'] = imdb['review'].apply(strip_html_tags)\n",
    "# imdb['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words, Punctuation, and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Cleaning up the text:\n",
    "def clean_text(text):\n",
    "    # Tokenizing the text\n",
    "    words = word_tokenize(text)\n",
    "    # Removing stop words, punctuation, and numbers \n",
    "    processed_words = [w for w in words if w.lower() not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    processed_words = ' '.join(processed_words)\n",
    "    return processed_words \n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(clean_text)\n",
    "# print(imdb['review']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Charcaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters:\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(remove_special_characters) \n",
    "# imdb['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Redundant Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the redundant whitespaces:\n",
    "def remove_redundant_whitespaces(text):\n",
    "    text = re.sub(pattern= r'\\s+', repl= ' ', string= text)\n",
    "    return text.strip()\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(remove_redundant_whitespaces)\n",
    "# print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Text Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the text:\n",
    "def simple_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
    "    return stemmed_text\n",
    "\n",
    "# imdb['review'] = imdb['review'].apply(simple_stemmer)\n",
    "# print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sando\\AppData\\Local\\Temp\\ipykernel_13392\\1919239933.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one review mention watch oz episod ll hook rig...\n",
      "1        wonder littl product film techniqu unassum old...\n",
      "2        thought wonder way spend time hot summer weeke...\n",
      "3        basic s famili littl boy jake think s zombi cl...\n",
      "4        petter mattei s love time money visual stun fi...\n",
      "                               ...                        \n",
      "49995    thought movi right good job nt creativ origin ...\n",
      "49996    bad plot bad dialogu bad act idiot direct anno...\n",
      "49997    cathol taught parochi elementari school nun ta...\n",
      "49998    m go disagre previou comment side maltin one s...\n",
      "49999    one expect star trek movi high art fan expect ...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    # Removing the HTML tags:\n",
    "    normalized_text = strip_html_tags(text)\n",
    "\n",
    "    # Removing stop words, punctuation, and numbers\n",
    "    normalized_text = clean_text(normalized_text)\n",
    "\n",
    "    # Removing special characters:\n",
    "    normalized_text = remove_special_characters(normalized_text, remove_digits = False)\n",
    "\n",
    "    # Removing the redundant whitespaces:\n",
    "    normalized_text = remove_redundant_whitespaces(normalized_text)\n",
    "\n",
    "    # Stemmatization:\n",
    "    normalized_text = simple_stemmer(normalized_text)\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "imdb['review'] = imdb['review'].apply(normalize_text)\n",
    "print(imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 35000\n",
      "x_test: 15000\n",
      "y_train: 35000\n",
      "y_test: 15000\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset using sklearn\n",
    "X = imdb['review'] # features\n",
    "y = imdb['sentiment'] # target labels\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.3)\n",
    "\n",
    "print('x_train:', x_train.shape[0])\n",
    "print('x_test:', x_test.shape[0])\n",
    "print('y_train:', y_train.shape[0])\n",
    "print('y_test:', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW_cv_train: (35000, 5969179)\n",
      "BoW_cv_test: (15000, 5969179)\n"
     ]
    }
   ],
   "source": [
    "# Count vectorizer for bag of words:\n",
    "cv = CountVectorizer(min_df= 0.0, # lower threshold\n",
    "                     max_df= 1.0, # upper threshold\n",
    "                     binary= False, \n",
    "                     ngram_range= (1,3)) # ngram_range(1, 3) includes more context\n",
    "# Transforming the training set:\n",
    "cv_train_reviews = cv.fit_transform(x_train)\n",
    "#Tranforming the testing set:\n",
    "cv_test_reviews = cv.transform(x_test)\n",
    "\n",
    "print('BoW_cv_train:', cv_train_reviews.shape)\n",
    "print('BoW_cv_test:', cv_test_reviews.shape)\n",
    "vocab = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Term Frequency-Inverse Document Frequency Model (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (35000, 5969179)\n",
      "Tfidf_test: (15000, 5969179)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(min_df= 0.0, max_df= 1.0, use_idf= True, ngram_range=(1,3))\n",
    "tv_train_reviews = tv.fit_transform(x_train)\n",
    "tv_test_reviews = tv.transform(x_test)\n",
    "\n",
    "print('Tfidf_train:', tv_train_reviews.shape)\n",
    "print('Tfidf_test:', tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow: LogisticRegression(C=1, max_iter=500, random_state=42)\n",
      "lr_tfidf: LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Training the logistic regression model:\n",
    "lr = LogisticRegression(penalty= 'l2', #* L2 Regularization (penalizes the sum of the squared values)\n",
    "                        max_iter= 500, #* maximum number of iterations\n",
    "                        C=1, #* regularization strength\n",
    "                        random_state= 42)\n",
    "\n",
    "# Fitting the model for BoW features:\n",
    "#* variable_name = lr.fit(x_train, y_train)\n",
    "lr_bow = lr.fit(cv_train_reviews, y_train)\n",
    "print('lr_bow:', lr_bow)\n",
    "\n",
    "# Fitting the model for TFIDF feature:\n",
    "#* variable_name = lr.fit(x_train, y_train)\n",
    "lr_tfidf = lr.fit(tv_train_reviews, y_train)\n",
    "print('lr_tfidf:', lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance on the Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predicting using BoW model:\n",
    "lr_bow_predictions = lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predictions)\n",
    "# Predicting using the TFIDF model:\n",
    "lr_tfidf_predictions = lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score: 0.8475333333333334\n",
      "lr_tfidf_score: 0.879\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for the bag of words features:\n",
    "#* accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "lr_bow_score = accuracy_score(y_test, lr_bow_predictions)\n",
    "print('lr_bow_score:', lr_bow_score)\n",
    "# Accuracy score for the tfidf features:\n",
    "#* accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "lr_tfidf_score = accuracy_score(y_test, lr_tfidf_predictions)\n",
    "print('lr_tfidf_score:', lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: rgb(50, 98, 168);\">Question: Why is my accuracy significantly higher than the example notebook I used?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Classification Report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Summary:\n",
    "#* Precision: Measures the accuracy of positive predictions.\n",
    "#* Recall: Measures how many of the actual positives were captured by the model.\n",
    "#* F1-score: Harmonic mean of precision and recall, providing a single metric for performance balance.\n",
    "#* Support: Number of instances in each class.\n",
    "#* Accuracy: Overall correctness of predictions.\n",
    "#* Macro Average: This is the average of the precision, recall, and F1-score across all classes. It gives equal weight to each class, regardless of support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_report:               precision    recall  f1-score   support\n",
      "\n",
      "     Postive       0.83      0.88      0.85      7523\n",
      "    Negative       0.87      0.81      0.84      7477\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.85      0.85      0.85     15000\n",
      "weighted avg       0.85      0.85      0.85     15000\n",
      "\n",
      "tfdif_report:               precision    recall  f1-score   support\n",
      "\n",
      "     Postive       0.89      0.87      0.88      7523\n",
      "    Negative       0.87      0.89      0.88      7477\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining our target names:\n",
    "target_names = ['Postive', 'Negative']\n",
    "\n",
    "# Classification report for the bag of words model:\n",
    "lr_bow_report =  classification_report(y_test, lr_bow_predictions, target_names = target_names)\n",
    "print('bow_report:', lr_bow_report)\n",
    "# Classification report for the tfidf model:\n",
    "lr_tfdif_report =  classification_report(y_test, lr_tfidf_predictions, target_names = ['Postive', 'Negative'])\n",
    "print('tfdif_report:', lr_tfdif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_confusion_matrix: \n",
      "\n",
      " [[6622  901]\n",
      " [1386 6091]] \n",
      "\n",
      "tfidf_confusion_matrix: \n",
      "\n",
      " [[6622  901]\n",
      " [1386 6091]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix for bow:\n",
    "bow_confusion_matrix = confusion_matrix(y_test, lr_bow_predictions, labels= [0, 1])\n",
    "print('bow_confusion_matrix: \\n\\n', bow_confusion_matrix, '\\n')\n",
    "# Confusion matrix for tfidf:\n",
    "tfidf_confusion_matrix = confusion_matrix(y_test, lr_bow_predictions, labels= [0, 1])\n",
    "print('tfidf_confusion_matrix: \\n\\n', tfidf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
